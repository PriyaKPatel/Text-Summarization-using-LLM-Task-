{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using LLM - Complete Demonstration\n",
    "\n",
    "**Assignment**: News Article Summarization with Production-Ready Models\n",
    "\n",
    "**Author**: Your Name  \n",
    "**Date**: January 2026\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. ‚úÖ Model Selection (BART-CNN for production)\n",
    "2. ‚úÖ Data Preprocessing (XSum dataset)\n",
    "3. ‚úÖ Pipeline Implementation\n",
    "4. ‚úÖ Model Comparison (BART vs PEGASUS vs LED)\n",
    "5. ‚úÖ Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers datasets torch pandas accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection & Justification\n",
    "\n",
    "### Primary Model: facebook/bart-large-cnn\n",
    "\n",
    "**Why BART-CNN? (Production-Focused)**\n",
    "\n",
    "| Criterion | BART-CNN | PEGASUS-XSum | LED-16K |\n",
    "|-----------|----------|--------------|----------|\n",
    "| **Training** | CNN/DailyMail | XSum (BBC) | Multi-domain |\n",
    "| **Output** | 3-4 sentences | 1 sentence | 3-4 sentences |\n",
    "| **Industry Use** | ‚úÖ Very High | ‚ö†Ô∏è Niche | ‚úÖ Specialized |\n",
    "| **Business Value** | ‚úÖ High | ‚ùå Low | ‚úÖ Medium |\n",
    "| **Speed** | Fast (1-2s) | Fast (1-2s) | Slower (3-5s) |\n",
    "| **Production Ready** | ‚úÖ Yes | ‚ö†Ô∏è Limited | ‚úÖ Yes (long docs) |\n",
    "\n",
    "**Decision**: BART-CNN is the industry standard for news summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"BART-CNN\": {\n",
    "        \"name\": \"facebook/bart-large-cnn\",\n",
    "        \"max_len\": 142,\n",
    "        \"min_len\": 56,\n",
    "        \"description\": \"Production standard - informative 3-4 sentence summaries\"\n",
    "    },\n",
    "    \"PEGASUS-XSum\": {\n",
    "        \"name\": \"google/pegasus-xsum\",\n",
    "        \"max_len\": 64,\n",
    "        \"min_len\": 10,\n",
    "        \"description\": \"Academic - single sentence extreme summarization\"\n",
    "    },\n",
    "    \"LED-16K\": {\n",
    "        \"name\": \"allenai/led-base-16384\",\n",
    "        \"max_len\": 142,\n",
    "        \"min_len\": 56,\n",
    "        \"description\": \"Specialized - handles very long documents\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìä Available Models:\")\n",
    "for name, config in MODELS.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Model: {config['name']}\")\n",
    "    print(f\"  Description: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing\n",
    "\n",
    "### Library Choices and Justification:\n",
    "\n",
    "**Why `datasets` (HuggingFace) over alternatives?**\n",
    "- **Native Integration**: Seamlessly works with HuggingFace models and tokenizers\n",
    "- **Efficient Loading**: Lazy loading and caching reduce memory footprint\n",
    "- **Built-in Support**: Direct access to XSum dataset without manual download/processing\n",
    "- **Alternative Considered**: Manual CSV/JSON loading - rejected due to complexity and lack of optimization\n",
    "\n",
    "**Why `pandas` for data manipulation?**\n",
    "- **Structured Data**: Excellent for tabular data operations and analysis\n",
    "- **DataFrame Operations**: Easy filtering, selection, and transformation\n",
    "- **Integration**: Works well with datasets library output\n",
    "- **Alternative Considered**: NumPy arrays - rejected as they lack structured data capabilities\n",
    "\n",
    "**Why `torch` (PyTorch)?**\n",
    "- **Model Backend**: Required by HuggingFace transformers library\n",
    "- **GPU Acceleration**: Automatic CUDA support for faster inference\n",
    "- **Tensor Operations**: Efficient numerical computations\n",
    "- **Alternative Considered**: TensorFlow - rejected as HuggingFace defaults to PyTorch for these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XSum dataset\n",
    "print(\"üì• Loading XSum dataset...\")\n",
    "dataset = load_dataset(\"xsum\", split=\"test\")\n",
    "samples = dataset.select(range(50))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(samples)} samples\")\n",
    "print(f\"\\nDataset structure: {samples.column_names}\")\n",
    "print(f\"First example keys: {samples[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample data\n",
    "print(\"üìä Sample Data Inspection:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Article (first 200 chars): {samples[i]['document'][:200]}...\")\n",
    "    print(f\"XSum Reference: {samples[i]['summary']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Implementation\n",
    "\n",
    "### Pipeline Abstraction:\n",
    "\n",
    "The HuggingFace `pipeline()` function abstracts away the following complexities:\n",
    "\n",
    "```\n",
    "Input Text ‚Üí Tokenization ‚Üí Model Inference ‚Üí Decoding ‚Üí Summary Output\n",
    "```\n",
    "\n",
    "**What complexities are abstracted?**\n",
    "\n",
    "1. **Tokenization**: \n",
    "   - Manual handling of tokenizers, special tokens, padding, truncation\n",
    "   - Without pipeline: Need to manually call `tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)`\n",
    "   - With pipeline: Automatically handles all tokenization steps\n",
    "\n",
    "2. **Model Loading**:\n",
    "   - Model weights, configuration, and tokenizer initialization\n",
    "   - Without pipeline: Need to load model, tokenizer, and config separately\n",
    "   - With pipeline: Single function call handles everything\n",
    "\n",
    "3. **Device Management**:\n",
    "   - CPU/GPU device placement and tensor movement\n",
    "   - Without pipeline: Manual `.to(device)` calls and device management\n",
    "   - With pipeline: Automatic device detection and optimization\n",
    "\n",
    "4. **Decoding**:\n",
    "   - Converting token IDs back to text, handling special tokens\n",
    "   - Without pipeline: Manual `tokenizer.decode()` with cleanup\n",
    "   - With pipeline: Clean text output automatically\n",
    "\n",
    "5. **Batch Processing**:\n",
    "   - Efficient batching, attention masks, padding\n",
    "   - Without pipeline: Complex batching logic required\n",
    "   - With pipeline: Simple list input, automatic batching\n",
    "\n",
    "**Why use pipeline over manual implementation?**\n",
    "- **Simplicity**: Reduces code from ~50 lines to 1 line\n",
    "- **Reliability**: Battle-tested implementation with error handling\n",
    "- **Flexibility**: Easy to switch between models without code changes\n",
    "- **Maintenance**: Updates to HuggingFace automatically improve our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BART-CNN (Primary Model)\n",
    "print(\"üöÄ Loading BART-CNN model...\")\n",
    "print(\"‚ö†Ô∏è  First run downloads ~2GB - subsequent runs are fast\\n\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "bart_summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ BART-CNN ready!\")\n",
    "print(f\"   Device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single Text Summarization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = samples[0]['document']\n",
    "xsum_ref = samples[0]['summary']\n",
    "\n",
    "print(\"üìÑ Input Article (first 400 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(test_text[:400] + \"...\\n\")\n",
    "\n",
    "# Generate summary\n",
    "print(\"‚è≥ Generating summary...\")\n",
    "start = time.time()\n",
    "\n",
    "result = bart_summarizer(\n",
    "    test_text,\n",
    "    max_length=142,\n",
    "    min_length=56,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "summary = result[0]['summary_text']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìù BART-CNN Summary (3-4 sentences):\")\n",
    "print(\"=\" * 80)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ XSum Reference (1 sentence):\")\n",
    "print(\"=\" * 80)\n",
    "print(xsum_ref)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input length: {len(test_text.split())} words\")\n",
    "print(f\"BART summary: {len(summary.split())} words\")\n",
    "print(f\"XSum reference: {len(xsum_ref.split())} words\")\n",
    "print(f\"Inference time: {elapsed:.2f} seconds\")\n",
    "print(f\"Compression ratio: {len(summary.split())/len(test_text.split())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison (BART vs PEGASUS vs LED)\n",
    "\n",
    "This demonstrates understanding of different model characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models for comparison\n",
    "print(\"üîÑ Loading models for comparison...\\n\")\n",
    "\n",
    "models_loaded = {}\n",
    "\n",
    "for name, config in MODELS.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    models_loaded[name] = pipeline(\n",
    "        \"summarization\",\n",
    "        model=config['name'],\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"  ‚úÖ {name} ready\")\n",
    "\n",
    "print(\"\\n‚úÖ All models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on same text\n",
    "comparison_results = []\n",
    "\n",
    "print(\"‚öñÔ∏è  Comparing Models on Same Article\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, model in models_loaded.items():\n",
    "    config = MODELS[model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start = time.time()\n",
    "    result = model(\n",
    "        test_text,\n",
    "        max_length=config['max_len'],\n",
    "        min_length=config['min_len'],\n",
    "        do_sample=False\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    summary = result[0]['summary_text']\n",
    "    \n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(f\"Length: {len(summary.split())} words\")\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': model_name,\n",
    "        'Summary': summary,\n",
    "        'Words': len(summary.split()),\n",
    "        'Time (s)': f\"{elapsed:.2f}\",\n",
    "        'Description': config['description']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\nüìä Model Comparison Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ BART-CNN: Longest, most informative (production choice)\")\n",
    "print(\"‚ö° PEGASUS-XSum: Shortest, single sentence (headline style)\")\n",
    "print(\"üìö LED-16K: Similar to BART, better for very long docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Demonstrates efficient processing of multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Batch Processing Demo\\n\")\n",
    "\n",
    "batch_docs = [samples[i]['document'] for i in range(10)]\n",
    "batch_refs = [samples[i]['summary'] for i in range(10)]\n",
    "\n",
    "print(f\"Processing {len(batch_docs)} documents...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "batch_results = bart_summarizer(\n",
    "    batch_docs,\n",
    "    max_length=142,\n",
    "    min_length=56,\n",
    "    do_sample=False,\n",
    "    batch_size=4  # Process 4 at a time\n",
    ")\n",
    "total_time = time.time() - start\n",
    "\n",
    "batch_summaries = [r['summary_text'] for r in batch_results]\n",
    "\n",
    "print(\"‚úÖ Batch processing complete!\\n\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Avg per document: {total_time/len(batch_docs):.2f}s\")\n",
    "print(f\"Throughput: {len(batch_docs)/total_time:.2f} docs/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.DataFrame({\n",
    "    'Article': [d[:100] + '...' for d in batch_docs[:5]],\n",
    "    'BART Summary': [s[:100] + '...' for s in batch_summaries[:5]],\n",
    "    'XSum Ref': batch_refs[:5],\n",
    "    'BART Words': [len(s.split()) for s in batch_summaries[:5]],\n",
    "    'Ref Words': [len(r.split()) for r in batch_refs[:5]]\n",
    "})\n",
    "\n",
    "print(\"\\nüìã Sample Results (first 5):\")\n",
    "print(\"=\" * 80)\n",
    "print(batch_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "metrics = {\n",
    "    'Average BART Length': mean([len(s.split()) for s in batch_summaries]),\n",
    "    'Average XSum Length': mean([len(r.split()) for r in batch_refs]),\n",
    "    'Compression Ratio': mean([\n",
    "        len(batch_summaries[i].split()) / len(batch_docs[i].split()) * 100\n",
    "        for i in range(len(batch_docs))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ BART produces {metrics['Average BART Length']/metrics['Average XSum Length']:.1f}x longer summaries than XSum\")\n",
    "print(\"‚úÖ More informative for business use\")\n",
    "print(\"‚úÖ Compresses original to ~{:.1f}% of original length\".format(metrics['Compression Ratio']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Document': batch_docs,\n",
    "    'BART_Summary': batch_summaries,\n",
    "    'XSum_Reference': batch_refs,\n",
    "    'BART_Words': [len(s.split()) for s in batch_summaries],\n",
    "    'XSum_Words': [len(r.split()) for r in batch_refs]\n",
    "})\n",
    "\n",
    "results_df.to_csv('summarization_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to 'summarization_results.csv'\")\n",
    "print(f\"   Total entries: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Conclusions\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "1. ‚úÖ **Model Selection**: BART-CNN chosen for production readiness\n",
    "2. ‚úÖ **Data Processing**: Efficient XSum dataset loading\n",
    "3. ‚úÖ **Pipeline**: Abstracted complexity with HuggingFace pipeline\n",
    "4. ‚úÖ **Comparison**: Demonstrated understanding of model tradeoffs\n",
    "5. ‚úÖ **Performance**: Efficient batch processing\n",
    "\n",
    "### Why BART-CNN?\n",
    "\n",
    "| Aspect | Reason |\n",
    "|--------|--------|\n",
    "| **Output Quality** | 3-4 informative sentences vs 1 brief sentence |\n",
    "| **Industry Use** | Most widely used summarization model |\n",
    "| **Reliability** | Battle-tested, predictable behavior |\n",
    "| **Business Value** | Suitable for reports, briefs, production use |\n",
    "\n",
    "### Production vs Academic:\n",
    "\n",
    "- **Academic Approach**: Match model to dataset (PEGASUS-XSum)\n",
    "- **Production Approach**: Choose best model for business needs (BART-CNN)\n",
    "- **Our Choice**: Production approach with academic awareness\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Run the Streamlit app: `streamlit run app.py`\n",
    "2. Try your own articles\n",
    "3. Compare models interactively\n",
    "4. Deploy for production use\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
