{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6j9_wMiA3uE"
      },
      "source": [
        "# Text Summarization using\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeiRGrmfA3uE"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "121mJeyoA3uE"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch pandas accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YytGAeprA3uF",
        "outputId": "b2be3abc-5a69-409f-d01c-814b635b62f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup complete!\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "Device: GPU\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEstJwvAA3uF"
      },
      "source": [
        "## 2. Model Selection & Justification\n",
        "\n",
        "### Selected Model: **facebook/bart-large-cnn** (BART-CNN)\n",
        "\n",
        "**Why BART-CNN?**\n",
        "- Produces informative 3-4 sentence summaries (vs PEGASUS-XSum's 1 sentence)\n",
        "- Trained on CNN/DailyMail dataset, ideal for news articles\n",
        "\n",
        "**Comparison:**\n",
        "| Criterion | BART-CNN | PEGASUS-XSum | LED-16K |\n",
        "|-----------|----------|--------------|----------|\n",
        "| Output | 3-4 sentences | 1 sentence | 3-4 sentences |\n",
        "| Business Value | High | Low | Medium |\n",
        "| Industry Use | Standard | Niche | Specialized |\n",
        "\n",
        "**Decision Rationale:**\n",
        "BART-CNN is the industry standard for news summarization. It produces informative 3-4 sentence summaries suitable for business use, making it more valuable than PEGASUS-XSum's single-sentence output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pmWEKNA3uF",
        "outputId": "22025e05-e062-4ac3-c765-f434c44473ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Available Models:\n",
            "\n",
            "BART-CNN:\n",
            "  Model: facebook/bart-large-cnn\n",
            "  Description: Production standard - informative 3-4 sentence summaries\n",
            "\n",
            "PEGASUS-XSum:\n",
            "  Model: google/pegasus-xsum\n",
            "  Description: Academic - single sentence extreme summarization\n",
            "\n",
            "LED-16K:\n",
            "  Model: allenai/led-base-16384\n",
            "  Description: Specialized - handles very long documents\n"
          ]
        }
      ],
      "source": [
        "# Model configurations\n",
        "MODELS = {\n",
        "    \"BART-CNN\": {\n",
        "        \"name\": \"facebook/bart-large-cnn\",\n",
        "        \"max_len\": 142,\n",
        "        \"min_len\": 56,\n",
        "        \"description\": \"Production standard - informative 3-4 sentence summaries\"\n",
        "    },\n",
        "    \"PEGASUS-XSum\": {\n",
        "        \"name\": \"google/pegasus-xsum\",\n",
        "        \"max_len\": 64,\n",
        "        \"min_len\": 10,\n",
        "        \"description\": \"Academic - single sentence extreme summarization\"\n",
        "    },\n",
        "    \"LED-16K\": {\n",
        "        \"name\": \"allenai/led-base-16384\",\n",
        "        \"max_len\": 142,\n",
        "        \"min_len\": 56,\n",
        "        \"description\": \"Specialized - handles very long documents\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìä Available Models:\")\n",
        "for name, config in MODELS.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Model: {config['name']}\")\n",
        "    print(f\"  Description: {config['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntsc0CIAA3uF"
      },
      "source": [
        "## 3. Data Loading & Preprocessing\n",
        "\n",
        "### Library Choices and Justification:\n",
        "\n",
        "**`datasets` (HuggingFace)**\n",
        "- Native integration with models/tokenizers\n",
        "- Efficient lazy loading and caching\n",
        "- Direct XSum dataset access\n",
        "- *Alternative:* Manual CSV/JSON - rejected (too complex)\n",
        "\n",
        "**`pandas`**\n",
        "- Structured data operations and analysis\n",
        "- Easy DataFrame filtering/transformation\n",
        "- *Alternative:* NumPy - rejected (lacks structured data)\n",
        "\n",
        "**`torch` (PyTorch)**\n",
        "- Required backend for HuggingFace transformers\n",
        "- Automatic GPU acceleration\n",
        "- *Alternative:* TensorFlow - rejected (HF defaults to PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwMFxGcSA3uF",
        "outputId": "855ebc93-ba04-43ad-d72e-f4af49239e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading XSum dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2f00601d-91a7-40dc-b49d-a9e6440fe800)')' thrown while requesting HEAD https://huggingface.co/datasets/xsum/resolve/7d4d486c2f8ef850b1a11aead99b894ff3dd7da9/dataset_infos.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2f00601d-91a7-40dc-b49d-a9e6440fe800)')' thrown while requesting HEAD https://huggingface.co/datasets/xsum/resolve/7d4d486c2f8ef850b1a11aead99b894ff3dd7da9/dataset_infos.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 50 samples\n",
            "\n",
            "Dataset structure: ['document', 'summary', 'id']\n",
            "First example keys: dict_keys(['document', 'summary', 'id'])\n"
          ]
        }
      ],
      "source": [
        "# Load XSum dataset\n",
        "print(\"üì• Loading XSum dataset...\")\n",
        "dataset = load_dataset(\"xsum\", split=\"test\")\n",
        "samples = dataset.select(range(50))  # 50 samples for demo\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(samples)} samples\")\n",
        "print(f\"\\nDataset structure: {samples.column_names}\")\n",
        "print(f\"First example keys: {samples[0].keys()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKPP5Ay1A3uF",
        "outputId": "10ea839a-6528-45d7-d30d-4430aee01262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Sample Data Inspection:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Article (first 200 chars): Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\n",
            "Workers at the charity claim investment in housing...\n",
            "XSum Reference: There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Article (first 200 chars): Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\n",
            "Detectives said three firearms, ammunition and a five-figure sum of money were recovered.\n",
            "A 26-yea...\n",
            "XSum Reference: A man has appeared in court after firearms, ammunition and cash were seized by police in Edinburgh.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Article (first 200 chars): Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Covington, 24, appeared in a Chicago court on Friday.\n",
            "The four have been charged with hate crimes and aggravated kidnapping and...\n",
            "XSum Reference: Four people accused of kidnapping and torturing a mentally disabled man in a \"racially motivated\" attack streamed on Facebook have been denied bail.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Inspect sample data\n",
        "print(\"üìä Sample Data Inspection:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Article (first 200 chars): {samples[i]['document'][:200]}...\")\n",
        "    print(f\"XSum Reference: {samples[i]['summary']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KvnRs1OA3uF"
      },
      "source": [
        "## 4. Pipeline Implementation\n",
        "\n",
        "### Pipeline Abstraction\n",
        "\n",
        "Used HuggingFace `pipeline()` to abstract model complexities:\n",
        "\n",
        "```\n",
        "Input Text ‚Üí Tokenization ‚Üí Model Inference ‚Üí Decoding ‚Üí Summary Output\n",
        "```\n",
        "\n",
        "**Complexities Abstracted:**\n",
        "1. **Tokenization:** Special tokens, padding, truncation\n",
        "2. **Model Loading:** Weights, config, tokenizer in single call\n",
        "3. **Device Management:** CPU/GPU auto-detection\n",
        "4. **Decoding:** Token IDs ‚Üí clean text output\n",
        "5. **Batch Processing:** Automatic batching with attention masks\n",
        "\n",
        "**Implementation:**\n",
        "```python\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
        "result = summarizer(text, max_length=142, min_length=56, do_sample=False)\n",
        "```\n",
        "\n",
        "**Benefits:** Reduces ~50 lines to 1 line, battle-tested, easy model switching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6IpJ7orA3uF",
        "outputId": "389a82da-4cfe-455a-fa70-f70c63656c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Loading BART-CNN model...\n",
            "‚ö†Ô∏è  First run downloads ~2GB - subsequent runs are fast\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BART-CNN ready!\n",
            "   Device: GPU\n"
          ]
        }
      ],
      "source": [
        "# Initialize BART-CNN (Primary Model)\n",
        "print(\"üöÄ Loading BART-CNN model...\")\n",
        "print(\"‚ö†Ô∏è  First run downloads ~2GB - subsequent runs are fast\\n\")\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "bart_summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"‚úÖ BART-CNN ready!\")\n",
        "print(f\"   Device: {'GPU' if device == 0 else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKWplOAbA3uF"
      },
      "source": [
        "## 5. Single Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Ub1PvxA3uF",
        "outputId": "ae0275be-bb32-4384-e55c-19b12fd518b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Input Article (first 400 chars):\n",
            "================================================================================\n",
            "Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\n",
            "Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\n",
            "The Welsh Government said more people than ever were getting help to address housing problems.\n",
            "Changes to the Housing Act in Wales, introduced...\n",
            "\n",
            "‚è≥ Generating summary...\n",
            "\n",
            "================================================================================\n",
            "üìù BART-CNN Summary (3-4 sentences):\n",
            "================================================================================\n",
            "Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders. Welsh Government said more people than ever were getting help to address housing problems.\n",
            "\n",
            "================================================================================\n",
            "üéØ XSum Reference (1 sentence):\n",
            "================================================================================\n",
            "There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\n",
            "\n",
            "================================================================================\n",
            "üìä Metrics:\n",
            "================================================================================\n",
            "Input length: 599 words\n",
            "BART summary: 54 words\n",
            "XSum reference: 17 words\n",
            "Inference time: 1.82 seconds\n",
            "Compression ratio: 9.0%\n"
          ]
        }
      ],
      "source": [
        "# Test with first example\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Check if model exists, reload if necessary\n",
        "if 'bart_summarizer' not in globals():\n",
        "    print(\"üîÑ Reloading BART-CNN model...\")\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    bart_summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=\"facebook/bart-large-cnn\",\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "test_text = samples[0]['document']\n",
        "xsum_ref = samples[0]['summary']\n",
        "\n",
        "print(\"üìÑ Input Article (first 400 chars):\")\n",
        "print(\"=\" * 80)\n",
        "print(test_text[:400] + \"...\\n\")\n",
        "\n",
        "# Generate summary\n",
        "print(\"‚è≥ Generating summary...\")\n",
        "start = time.time()\n",
        "\n",
        "# Added truncation=True to prevent index out of bounds errors\n",
        "result = bart_summarizer(\n",
        "    test_text,\n",
        "    max_length=142,\n",
        "    min_length=56,\n",
        "    do_sample=False,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "summary = result[0]['summary_text']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù BART-CNN Summary (3-4 sentences):\")\n",
        "print(\"=\" * 80)\n",
        "print(summary)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ XSum Reference (1 sentence):\")\n",
        "print(\"=\" * 80)\n",
        "print(xsum_ref)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Metrics:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Input length: {len(test_text.split())} words\")\n",
        "print(f\"BART summary: {len(summary.split())} words\")\n",
        "print(f\"XSum reference: {len(xsum_ref.split())} words\")\n",
        "print(f\"Inference time: {elapsed:.2f} seconds\")\n",
        "print(f\"Compression ratio: {len(summary.split())/len(test_text.split())*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pPxwne7A3uF"
      },
      "source": [
        "## 6. Model Comparison (BART vs PEGASUS vs LED)\n",
        "\n",
        "This demonstrates understanding of different model characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufegT288A3uF",
        "outputId": "3c3dd015-a007-4fff-f5a7-dae229979cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Preparing for model comparison...\n",
            "To save GPU memory, we will load models sequentially in the next cell rather than all at once.\n",
            "Unloading initial BART model to free space...\n",
            "‚úÖ Memory cleared. Ready for sequential comparison.\n"
          ]
        }
      ],
      "source": [
        "# Prepare for sequential comparison (Save Memory)\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Preparing for model comparison...\")\n",
        "print(\"To save GPU memory, we will load models sequentially in the next cell rather than all at once.\")\n",
        "\n",
        "# Clear GPU memory from previous cells to avoid OOM\n",
        "if 'bart_summarizer' in globals():\n",
        "    print(\"Unloading initial BART model to free space...\")\n",
        "    del bart_summarizer\n",
        "\n",
        "if 'models_loaded' in globals():\n",
        "    del models_loaded\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"‚úÖ Memory cleared. Ready for sequential comparison.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Memory clear failed: {e}\")\n",
        "    print(\"‚ùó CRITICAL: The GPU is in a bad state (Device-side assert). You MUST restart the runtime (Runtime > Restart session) to continue.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ9McUp_A3uF",
        "outputId": "7f81716e-cc2d-4426-dd18-6a77cdb96d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öñÔ∏è  Comparing Models on Same Article (Sequential Load/Unload)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "BART-CNN:\n",
            "--------------------------------------------------------------------------------\n",
            "Loading BART-CNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders. Welsh Government said more people than ever were getting help to address housing problems.\n",
            "Length: 54 words\n",
            "Time: 1.26s\n",
            "\n",
            "PEGASUS-XSum:\n",
            "--------------------------------------------------------------------------------\n",
            "Loading PEGASUS-XSum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: More than 1,000 homeless people were referred to a charity in Wales last year.\n",
            "Length: 14 words\n",
            "Time: 0.48s\n",
            "\n",
            "LED-16K:\n",
            "--------------------------------------------------------------------------------\n",
            "Loading LED-16K...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 1024\n",
            "Both `max_new_tokens` (=256) and `max_length`(=142) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.Image copyright PAWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.Image copyright PAThe Welsh Government said more people than ever were getting help to address housing problems.Changes to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.Image copyright PAPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.However, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.Image copyright PAAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".Image copyright PA\"There's a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.Image copyright PA\"It could take six months to a year\n",
            "Length: 197 words\n",
            "Time: 2.53s\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare on same text using Sequential Loading\n",
        "comparison_results = []\n",
        "\n",
        "print(\"‚öñÔ∏è  Comparing Models on Same Article (Sequential Load/Unload)\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model_name, config in MODELS.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        # Load model just-in-time\n",
        "        current_model = pipeline(\n",
        "            \"summarization\",\n",
        "            model=config['name'],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        start = time.time()\n",
        "        # Added truncation=True to prevent CUDA asserts on long docs\n",
        "        result = current_model(\n",
        "            test_text,\n",
        "            max_length=config['max_len'],\n",
        "            min_length=config['min_len'],\n",
        "            do_sample=False,\n",
        "            truncation=True\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        summary = result[0]['summary_text']\n",
        "\n",
        "        print(f\"Summary: {summary}\")\n",
        "        print(f\"Length: {len(summary.split())} words\")\n",
        "        print(f\"Time: {elapsed:.2f}s\")\n",
        "\n",
        "        comparison_results.append({\n",
        "            'Model': model_name,\n",
        "            'Summary': summary,\n",
        "            'Words': len(summary.split()),\n",
        "            'Time (s)': f\"{elapsed:.2f}\",\n",
        "            'Description': config['description']\n",
        "        })\n",
        "\n",
        "        # IMMEDIATE CLEANUP to save memory\n",
        "        del current_model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during summarization with {model_name}: {e}\")\n",
        "        comparison_results.append({\n",
        "            'Model': model_name,\n",
        "            'Summary': 'Error: Could not generate summary',\n",
        "            'Words': 0,\n",
        "            'Time (s)': 'N/A',\n",
        "            'Description': config['description']\n",
        "        })\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suwpmI1rA3uF",
        "outputId": "0a5c132a-da7d-4110-8c9b-60980ca0f9b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Model Comparison Table:\n",
            "================================================================================\n",
            "       Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Summary  Words Time (s)                                              Description\n",
            "    BART-CNN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders. Welsh Government said more people than ever were getting help to address housing problems.     54     1.26 Production standard - informative 3-4 sentence summaries\n",
            "PEGASUS-XSum                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                More than 1,000 homeless people were referred to a charity in Wales last year.     14     0.48         Academic - single sentence extreme summarization\n",
            "     LED-16K Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.Image copyright PAWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.Image copyright PAThe Welsh Government said more people than ever were getting help to address housing problems.Changes to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.Image copyright PAPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.However, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.Image copyright PAAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".Image copyright PA\"There's a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.Image copyright PA\"It could take six months to a year    197     2.53                Specialized - handles very long documents\n",
            "\n",
            "üí° Key Insights:\n",
            "================================================================================\n",
            "BART-CNN: Longest, most informative (production choice)\n",
            "‚ö° PEGASUS-XSum: Shortest, single sentence (headline style)\n",
            "LED-16K: Similar to BART, better for very long docs\n"
          ]
        }
      ],
      "source": [
        "# Display comparison table\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(\"\\nüìä Model Comparison Table:\")\n",
        "print(\"=\" * 80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"BART-CNN: Longest, most informative (production choice)\")\n",
        "print(\"‚ö° PEGASUS-XSum: Shortest, single sentence (headline style)\")\n",
        "print(\"LED-16K: Similar to BART, better for very long docs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpd1Y26JA3uG"
      },
      "source": [
        "## 7. Batch Processing\n",
        "\n",
        "Demonstrates efficient processing of multiple documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7hC-dv4A3uG",
        "outputId": "85277997-cc32-4b39-d9ec-c3609cf86325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Batch Processing Demo\n",
            "\n",
            "‚ö†Ô∏è GPU Error detected: CUDA error: device-side assert triggered\n",
            "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "‚ö†Ô∏è GPU appears corrupted. Switching to CPU mode to continue (slower but stable)...\n",
            "üîß Running on: CPU\n",
            "üîÑ (Re)Loading BART-CNN model and tokenizer on CPU...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing 10 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 142, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
            "Your max_length is set to 142, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Batch processing complete!\n",
            "\n",
            "Total time: 55.83s\n",
            "Avg per document: 5.58s\n",
            "Throughput: 0.18 docs/second\n"
          ]
        }
      ],
      "source": [
        "# Batch processing with BART\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "print(\"üì¶ Batch Processing Demo\\n\")\n",
        "\n",
        "# Flag to track if we need to fallback to CPU due to GPU errors\n",
        "force_cpu = False\n",
        "\n",
        "# 1. Try to clear GPU memory to check health\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è GPU Error detected: {e}\")\n",
        "    print(\"‚ö†Ô∏è GPU appears corrupted. Switching to CPU mode to continue (slower but stable)...\")\n",
        "    force_cpu = True\n",
        "\n",
        "# 2. Determine Device (CPU fallback if GPU is broken)\n",
        "device = 0 if (torch.cuda.is_available() and not force_cpu) else -1\n",
        "print(f\"üîß Running on: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# 3. Reload Model with Explicit Tokenizer\n",
        "print(f\"üîÑ (Re)Loading BART-CNN model and tokenizer on {'GPU' if device==0 else 'CPU'}...\")\n",
        "try:\n",
        "    # Load tokenizer explicitly to ensure max_length is set\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    # Enforce BART's hard limit\n",
        "    tokenizer.model_max_length = 1024\n",
        "\n",
        "    # Clean up old model variable if it exists\n",
        "    if 'bart_summarizer' in globals():\n",
        "        del bart_summarizer\n",
        "        gc.collect()\n",
        "\n",
        "    bart_summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=\"facebook/bart-large-cnn\",\n",
        "        tokenizer=tokenizer,\n",
        "        device=device\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Primary Load Failed: {e}\")\n",
        "    print(\"‚ö†Ô∏è Force-switching to CPU...\")\n",
        "    device = -1\n",
        "\n",
        "    # Reload on CPU\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    tokenizer.model_max_length = 1024\n",
        "\n",
        "    bart_summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=\"facebook/bart-large-cnn\",\n",
        "        tokenizer=tokenizer,\n",
        "        device=-1\n",
        "    )\n",
        "\n",
        "# Get 10 documents\n",
        "batch_docs = [samples[i]['document'] for i in range(10)]\n",
        "batch_refs = [samples[i]['summary'] for i in range(10)]\n",
        "\n",
        "print(f\"\\nProcessing {len(batch_docs)} documents...\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "try:\n",
        "    batch_results = bart_summarizer(\n",
        "        batch_docs,\n",
        "        max_length=142,\n",
        "        min_length=56,\n",
        "        do_sample=False,\n",
        "        batch_size=1,\n",
        "        truncation=True\n",
        "    )\n",
        "    total_time = time.time() - start\n",
        "\n",
        "    batch_summaries = [r['summary_text'] for r in batch_results]\n",
        "\n",
        "    print(\"‚úÖ Batch processing complete!\\n\")\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    print(f\"Avg per document: {total_time/len(batch_docs):.2f}s\")\n",
        "    print(f\"Throughput: {len(batch_docs)/total_time:.2f} docs/second\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Processing failed: {e}\")\n",
        "    print(\"Tips: Check if tokenizer.model_max_length is set correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brbtdYs4A3uG",
        "outputId": "b6b0a94e-d4cb-435d-d6c1-464dd97b6301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Sample Results (first 5):\n",
            "================================================================================\n",
            "                                                                                                 Article                                                                                            BART Summary                                                                                                                                             XSum Ref  BART Words  Ref Words\n",
            " Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up... Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up...                                                      There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.          54         17\n",
            " Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday... Officers searched properties in the Waterfront Park and Colonsay View areas of the city. Detectives ...                                                  A man has appeared in court after firearms, ammunition and cash were seized by police in Edinburgh.          42         17\n",
            " Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Covington, 24, appeared in ... Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Cunnington, 24, appeared in... Four people accused of kidnapping and torturing a mentally disabled man in a \"racially motivated\" attack streamed on Facebook have been denied bail.          47         23\n",
            "The 48-year-old former Arsenal goalkeeper played for the Royals for four years.\\nHe was appointed you... The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed you...                                           West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.          51         15\n",
            " Restoring the function of the organ - which helps control blood sugar levels - reversed symptoms of ... Diet reversed symptoms of type 1 and type 2 diabetes in animal experiments. Scientists say it could ...                                               The pancreas can be triggered to regenerate itself through a type of fasting diet, say US researchers.          50         17\n"
          ]
        }
      ],
      "source": [
        "# Display batch results\n",
        "if 'batch_summaries' in locals() and 'batch_refs' in locals() and 'batch_docs' in locals():\n",
        "    batch_df = pd.DataFrame({\n",
        "        'Article': [d[:100] + '...' for d in batch_docs[:5]],\n",
        "        'BART Summary': [s[:100] + '...' for s in batch_summaries[:5]],\n",
        "        'XSum Ref': batch_refs[:5],\n",
        "        'BART Words': [len(s.split()) for s in batch_summaries[:5]],\n",
        "        'Ref Words': [len(r.split()) for r in batch_refs[:5]]\n",
        "    })\n",
        "\n",
        "    print(\"\\nüìã Sample Results (first 5):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(batch_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Batch results not available. Please run the Batch Processing cell successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r90FTfrA3uG"
      },
      "source": [
        "## 8. Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWHuTWbqA3uG",
        "outputId": "90620675-a980-412f-dc95-eb6469592d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Performance Metrics:\n",
            "================================================================================\n",
            "Average BART Length: 49.50\n",
            "Average XSum Length: 21.10\n",
            "Compression Ratio: 26.55\n",
            "\n",
            "üí° Analysis:\n",
            "================================================================================\n",
            "‚úÖ BART produces 2.3x longer summaries than XSum\n",
            "‚úÖ More informative for business use\n",
            "‚úÖ Compresses original to ~26.5% of original length\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics\n",
        "from statistics import mean\n",
        "\n",
        "if 'batch_summaries' in locals() and batch_summaries:\n",
        "    metrics = {\n",
        "        'Average BART Length': mean([len(s.split()) for s in batch_summaries]),\n",
        "        'Average XSum Length': mean([len(r.split()) for r in batch_refs]),\n",
        "        'Compression Ratio': mean([\n",
        "            len(batch_summaries[i].split()) / len(batch_docs[i].split()) * 100\n",
        "            for i in range(len(batch_docs))\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìä Performance Metrics:\")\n",
        "    print(\"=\" * 80)\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    print(\"\\nüí° Analysis:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚úÖ BART produces {metrics['Average BART Length']/metrics['Average XSum Length']:.1f}x longer summaries than XSum\")\n",
        "    print(\"‚úÖ More informative for business use\")\n",
        "    print(\"‚úÖ Compresses original to ~{:.1f}% of original length\".format(metrics['Compression Ratio']))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Metrics cannot be calculated because batch summaries are missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW8PQMuAA3uG"
      },
      "source": [
        "## 9. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rp4WYPWA3uG",
        "outputId": "619cbceb-a768-489d-f4f5-d7d00ac8df39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Results saved to 'summarization_results.csv'\n",
            "   Total entries: 10\n"
          ]
        }
      ],
      "source": [
        "# Save comprehensive results\n",
        "if 'batch_summaries' in locals() and batch_summaries:\n",
        "    results_df = pd.DataFrame({\n",
        "        'Document': batch_docs,\n",
        "        'BART_Summary': batch_summaries,\n",
        "        'XSum_Reference': batch_refs,\n",
        "        'BART_Words': [len(s.split()) for s in batch_summaries],\n",
        "        'XSum_Words': [len(r.split()) for r in batch_refs]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv('summarization_results.csv', index=False)\n",
        "    print(\"‚úÖ Results saved to 'summarization_results.csv'\")\n",
        "    print(f\"   Total entries: {len(results_df)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to save.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
